{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nietzsche-lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finiteautomata/ml-examples/blob/master/notebooks/keras/nietzsche_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8mHx_E0gLcu-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras LSTM Text generation\n",
        "\n",
        "We use [this as inspiration](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)\n",
        "\n",
        "We will generate text trained with Nietzsche's writings.\n",
        "\n",
        "At least 20 epochs are required before the generated text\n",
        "starts sounding coherent.\n",
        "\n",
        "If you try this script on new data, make sure your corpus\n",
        "has at least ~100k characters. ~1M is better.\n"
      ]
    },
    {
      "metadata": {
        "id": "qYV03vG7Lf08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea73ff6e-3db9-45db-d89d-31f280102e09"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jlbosNbWLq1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18ea5653-d351-46f4-8a0b-87052fb100e1"
      },
      "cell_type": "code",
      "source": [
        "path = get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "    \n",
        "print(\"Length: {}\".format(len(text)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S7fy9uNaLtjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "13ce0b69-969a-4156-8ab2-abadfa49824a"
      },
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preface\n",
            "\n",
            "\n",
            "supposing that truth is a woman--what then? is there not ground\n",
            "for suspecting that all ph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsPTpajTLv3k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will be using a char-level LSTM network to generate the text:"
      ]
    },
    {
      "metadata": {
        "id": "oQxdE42pMSpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00788fcb-065a-4c7f-9e21-4598724008da"
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HBMMu80WNJFE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our training instances will be pieces of 40 characters, and the target will be the next character. We will select them in a moving-average fashion, with a step of 3 characters."
      ]
    },
    {
      "metadata": {
        "id": "dvW58X_hMS_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d756fc3d-1a41-4d52-abd9-65e9edaab521"
      },
      "cell_type": "code",
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 200285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J6sV_MqdQeky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "e1acaa4a-46d2-4d7e-f169-05138e1add4c"
      },
      "cell_type": "code",
      "source": [
        "print(\"X[0] ---> \", sentences[0])\n",
        "print(\"Y[0] ---> \", next_chars[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X[0] --->  preface\n",
            "\n",
            "\n",
            "supposing that truth is a woma\n",
            "Y[0] --->  n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZTzCMA_KNeZn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's convert this intro a tensor.\n",
        "\n",
        "We have 200285 sentences of 40 characters each. To convert this into a one-hot representation, we will convert them into a tensor of $200285 \\times 40 \\times 57$, where 57 is the number of different characters in our text.\n",
        "\n",
        "Also, $Y$ will be of dimension $200285 \\times 57$"
      ]
    },
    {
      "metadata": {
        "id": "69M4DFuTNCAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d184ab60-3836-44b0-9158-193d08fe8a60"
      },
      "cell_type": "code",
      "source": [
        "print('Vectorization...')\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "   \n",
        "print(\"X shape = \", X.shape)\n",
        "print(\"Y shape = \", y.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "X shape =  (200285, 40, 57)\n",
            "Y shape =  (200285, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V7c0QITxNFyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "352901f0-d790-46e7-d01e-ad5e9e05f683"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import CuDNNLSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9CFlinc2tIi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Sampling\n",
        "\n",
        "We will use a couple of strategies to sample random Nietzschean phrases ([see this link for more info](https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f))\n",
        "\n",
        "- Random Sampling: Just sample from the output of the softmax \n",
        "- Temperature Sampling: Apply a temperature before sampling\n",
        "\n",
        "Observation:\n",
        "\n",
        "Sampling with temperature equal to $1.0$ is the same as sampling from the output of the softmax"
      ]
    },
    {
      "metadata": {
        "id": "gKZtFRAzNcB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1477
        },
        "outputId": "af8dc97f-d893-4626-b859-856232c5bf56"
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    \n",
        "    for i in range(400):\n",
        "        # Generate the sentence vector\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "    \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, 0.4)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(X, y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200285/200285 [==============================] - 31s 153us/step - loss: 1.9860\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"aggot and stake, or of the bull-fight,\n",
            "t\"\n",
            "aggot and stake, or of the bull-fight,\n",
            "the form of the somethered more and the\n",
            "most be one simple consequences as there it delicated and interperting the seesed as the more more one unidigion of\n",
            "the consequently in the former to the foren the religions of the most as a solitical and for his at the deciesting a the suspections of the fore be made of the self-dount and the seess to the present, and religions of master the most as a most t\n",
            "Epoch 2/10\n",
            "200285/200285 [==============================] - 29s 145us/step - loss: 1.6262\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"\n",
            "of flight, a means and artifice for wit\"\n",
            "\n",
            "of flight, a means and artifice for with which is the way and persans and the reflection and the strong and the conscience of the strangle of the mans of the conscientument as the same and all man could the from one still as the strinedce and the struct of the stall that the man of the strong and the way the wast the belief to be the man of the stands as the strinding and same and and the man and the act of man the land of all man for \n",
            "Epoch 3/10\n",
            "200285/200285 [==============================] - 29s 145us/step - loss: 1.5396\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"e just the man that he is--which\n",
            "schopen\"\n",
            "e just the man that he is--which\n",
            "schopenhauence of the great of the intellingly and the made the polse the world, the necession of the notion to every the sensition of the man of all his of the sailt and distinctive and all this mankind the instincting to the stand the free the most sight and and that is one problems in the free who has been prose and so happiness and intellited the sight and his say the self-said and reals, and and her\n",
            "Epoch 4/10\n",
            "200285/200285 [==============================] - 29s 146us/step - loss: 1.4935\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"ly,\n",
            "as has been hinted, that a scientifi\"\n",
            "ly,\n",
            "as has been hinted, that a scientificism, and the strange of the still distruth and as a strengle of all with the movement of the more to the stands of the more form and the religious close without the more subject of man of the might and such a sound and of the subject, and it is a sound the subject of the men of the strength of the probably belook of the more of the preval to the strange of the acting of his result the desire of c\n",
            "Epoch 5/10\n",
            "200285/200285 [==============================] - 29s 147us/step - loss: 1.4627\n",
            "\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"the people: \"i must,\n",
            "therefore, make mys\"\n",
            "the people: \"i must,\n",
            "therefore, make myself form and primitate, who has not only which the continual and spirits and explained the sublement of the ascent and and sublime the extent and the were of all the stander which the religion of the prease the greatest and as a strength of the standing and with himself he who has the subjection of the desirable or and standing the contral all the sign of the philosophy of the present and can a st\n",
            "Epoch 6/10\n",
            "200285/200285 [==============================] - 29s 144us/step - loss: 1.4396\n",
            "\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"uch of the charm\n",
            "of the mysterious, the \"\n",
            "uch of the charm\n",
            "of the mysterious, the assertion of himself in the depert for string to man, and intercourse of a subsections, and as a propers of profound practer of the englishis is a man who would be souls, and stands as a stands of the stands and as a philosophical the more has not the fact of the feelings of master and attenting the sentence of man and in the final stands of the world and stands as a thing of the origination of mo\n",
            "Epoch 7/10\n",
            "200285/200285 [==============================] - 29s 145us/step - loss: 1.4219\n",
            "\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"to the conception of \"greatness\" to be n\"\n",
            "to the conception of \"greatness\" to be not all the more to the same the ears of a new willing a man of the same that the so the result of the contrary and lacking to the sense of the most apperious the sense of the perhaps himself as a new seems to make the sension of the contral perhaps a seems to the more sighte to possible to the pression of the action of a thing the world and fact to the profound word and man and profound the fact t\n",
            "Epoch 8/10\n",
            "200285/200285 [==============================] - 29s 144us/step - loss: 1.4090\n",
            "\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"uld anything originate out of its opposi\"\n",
            "uld anything originate out of its opposite and constitutes of the problem of comprehension of a thing in a consequence of the propersome of a higher of the strength of the schopenhauer of the same dependent to the most perhaps constions\n",
            "of an and the person want to the power, and the still good that is a very order to the same thing can be in their spirit with an and not and perhaps in the same of the spirit and things and spirits and i\n",
            "Epoch 9/10\n",
            "200285/200285 [==============================] - 29s 143us/step - loss: 1.3984\n",
            "\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \"s\n",
            "life on the field of his fatherland's \"\n",
            "s\n",
            "life on the field of his fatherland's and who call as the first call as the sensation of the fact that the fact that the person of a man of the content and as the sense and subject that the moral true the instinct to man of the sense of the consideration of his master, so that the according to the same to the sense and the delight of the former the some of the sense of the more charm a far to make the world, and has really the will wi\n",
            "Epoch 10/10\n",
            "200285/200285 [==============================] - 29s 143us/step - loss: 1.3874\n",
            "\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"is, has perhaps just thereby, without\n",
            "re\"\n",
            "is, has perhaps just thereby, without\n",
            "religious and the part of the most philosophers of the mowarical of the merely the seeling in the word is to a constious and the really to the most subject of the most pain of the sense of it is to the most for the fact that the significance of the most person and the opposite of man and which is the and man and for the presence of the fore, it is to be formerly all the most conscience and understoo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6a05854c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "1eoxxPp2RKxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
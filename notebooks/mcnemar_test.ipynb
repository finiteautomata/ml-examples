{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Classifiers \n",
    "\n",
    "In this notebook, we use the `statsmodels'` implementation of `McNemar` test. This statistical tool serves to assess whether two classifiers perform \"equally\" well over a test set.\n",
    "\n",
    "First, let's load the Breast Cancer Dataset. We will construct two RandomForest with 50 and 51 estimators with the hope that there is no real difference in their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the classifiers and print their accuracy scores over the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877\n",
      "0.947368421053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "clf_A = RandomForestClassifier(n_estimators=50)\n",
    "clf_B = RandomForestClassifier(n_estimators=51)\n",
    "\n",
    "clf_A.fit(X_train, Y_train);\n",
    "clf_B.fit(X_train, Y_train);\n",
    "\n",
    "print clf_A.score(X_test, Y_test)\n",
    "print clf_B.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The difference here is not huge, so we expect that the McNemar test doesn't reject the null hypothesis of the two classifiers performing equally well (i.e. having the same error rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-Value : 1.0\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.sandbox.stats.runs import mcnemar\n",
    "\n",
    "stats, pval = mcnemar(clf_A.predict(X_test), clf_B.predict(X_test), exact=True)\n",
    "\n",
    "print \"P-Value : {}\".format(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The p-value is well enough above 0.05, so we don't reject $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for different classifiers\n",
    "\n",
    "In this case, we expect to see a difference between a RandomForest and a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877\n",
      "0.649122807018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "clf_A = RandomForestClassifier(n_estimators=50)\n",
    "clf_B = BernoulliNB()\n",
    "\n",
    "clf_A.fit(X_train, Y_train);\n",
    "clf_B.fit(X_train, Y_train);\n",
    "\n",
    "print clf_A.score(X_test, Y_test)\n",
    "print clf_B.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-Value : 2.27373675443e-13\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.sandbox.stats.runs import mcnemar\n",
    "\n",
    "stats, pval = mcnemar(clf_A.predict(X_test), clf_B.predict(X_test), exact=True)\n",
    "\n",
    "print \"P-Value : {}\".format(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the p-value shows that there is evidence of the two classifiers not performing identically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
